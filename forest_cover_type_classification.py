# -*- coding: utf-8 -*-
"""Forest_Cover_Type_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CeEFOgzP9rV8L92P0b1DBSpbUUW3G85q
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data =pd.read_csv('/content/covtype.csv')
data.head()

data.info()

data['Cover_Type'].nunique()

plt.figure(figsize=(8,5))
sns.countplot(x="Cover_Type", data=data)
plt.title("Target Distribution: Cover_Type")
plt.show()

data.nunique().sort_values()

# continuous features
cont_features = [
    "Elevation", "Aspect", "Slope",
    "Horizontal_Distance_To_Hydrology", "Vertical_Distance_To_Hydrology",
    "Horizontal_Distance_To_Roadways", "Horizontal_Distance_To_Fire_Points",
    "Hillshade_9am", "Hillshade_Noon", "Hillshade_3pm"
]

plt.figure(figsize=(15, 20))

for i, col in enumerate(cont_features, 1):
    # Histogram
    plt.subplot(len(cont_features), 2, 2*i-1)
    sns.histplot(data[col], bins=30, kde=True, color="skyblue")
    plt.title(f"Histogram - {col}")

    # Boxplot
    plt.subplot(len(cont_features), 2, 2*i)
    sns.boxplot(x=data[col], color="orange")
    plt.title(f"Boxplot - {col}")

plt.tight_layout()
plt.show()

# Correlation heatmap for continuous features only
plt.figure(figsize=(10,8))
corr = data[cont_features].corr()

sns.heatmap(corr, annot=True, cmap="coolwarm", center=0)
plt.title("Correlation Heatmap - Continuous Features")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score, f1_score
from xgboost import XGBClassifier

X =data.drop("Cover_Type", axis=1)
y =data['Cover_Type'] -1

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

class_counts = y_train.value_counts().sort_index()
class_weights = {i: len(y_train) / (len(class_counts) * class_counts[i]) for i in class_counts.index}
print("Class Weights:", class_weights)

sample_weights = y_train.map(class_weights)

model = XGBClassifier(
    objective="multi:softmax",
    num_class=len(class_counts),
    eval_metric="mlogloss",
    random_state=42,
    n_jobs=-1
)

model.fit(X_train, y_train, sample_weight=sample_weights)

y_pred = model.predict(X_test)

print("Balanced Accuracy:", balanced_accuracy_score(y_test, y_pred))
print("Macro F1:", f1_score(y_test, y_pred, average="macro"))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

import matplotlib.pyplot as plt
import xgboost as xgb

xgb.plot_importance(model, max_num_features=15, importance_type="gain")
plt.show()

param_dist = {
    "n_estimators": [100, 200],
    "max_depth": [10, 20],
    "max_features": ["sqrt", None]
}

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state=42, n_jobs=-1)

random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_dist,
    n_iter=3,
    scoring="f1_macro",
    cv=2,
    random_state=42,
    verbose=2,
    n_jobs=-1
)

random_search.fit(X_train, y_train)

print("Best Parameters:", random_search.best_params_)

y_pred = random_search.best_estimator_.predict(X_test)

print("Balanced Accuracy:", balanced_accuracy_score(y_test, y_pred))
print("Macro F1:", f1_score(y_test, y_pred, average="macro"))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

from sklearn.metrics import ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - Heatmap")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()